<p>The following guides will be useful when preparing your artifact:</p>
<ul>
  <li><a href="https://docs.google.com/document/d/1pqzPtLVIvwLwJsZwCb2r7yzWMaifudHe1Xvn42T4CcA/edit">HOWTO for AEC Submitters</a>,
by Dan Barowy, Charlie Curtsinger, Emma Tosch, John Vilk, and Emery Berger</li>
  <li><a href="https://blog.padhye.org/Artifact-Evaluation-Tips-for-Authors/">Artifact Evaluation: Tips for Authors</a>,
by Rohan Padhye</li>
</ul>

<p>Here are some general tips to make life easier for both artifact authors and evaluators:</p>

<ul>
  <li>
    <p><strong>Automate as much as possible</strong>, you will save a lot of time in the end from not having to re-run experiments that suffered from human error.
This is feasible even for artifacts that need multiple nodes or to replicate configuration in multiple places.
See <a href="https://github.com/SolalPirelli/docker-artifact-eval">this repository</a> for an example of artifact automation with Docker.</p>
  </li>
  <li>
    <p><strong>Try out your own artifact on a blank environment</strong>, following the steps you documented.
One lightweight way to do this is to create a Docker container from a base OS image, such as <code class="language-plaintext highlighter-rouge">ubuntu:latest</code>.
You can also use a virtual machine or even provision a real machine if you have the infrastructure to do so.</p>
  </li>
  <li>
    <p><strong>Log both successes and failures</strong>, so that users know that something happened.
Avoid logging unnecessary or confusing information, such as verbose output or failures that are actually expected.
Log potential issues, such as an optional but recommended library not being present.</p>
  </li>
  <li>
    <p><strong>Measure resource use</strong> using tools such as <code class="language-plaintext highlighter-rouge">mpstat</code>, <code class="language-plaintext highlighter-rouge">iostat</code>, <code class="language-plaintext highlighter-rouge">vmstat</code>, and <code class="language-plaintext highlighter-rouge">ifstat</code> to measure CPU, I/O, memory, and network use respectively on Linux,
or <code class="language-plaintext highlighter-rouge">/usr/bin/time -v</code> to measures the time and memory used by a command also on Linux.
This lets users know what to expect.</p>
  </li>
</ul>

<h2 id="checklists">Checklists</h2>

<p>Unfortunately, artifacts sometimes miss badges because they were not tested on a
clean setup, or not documented enough, or because running experiments is too
error-prone due to complex manual steps. <strong>This year, we provide checklists for authors and evaluators</strong>
to help prepare and evaluate artifacts, minimizing the
risk of an artifact unnecessarily missing a badge.</p>

<h3 id="artifact-available">Artifact Available</h3>

<ul>
  <li>The artifact is available on a public website with a specific version such as a git commit</li>
  <li>The artifact has a “read me” file with a reference to the paper</li>
  <li>Ideally, the artifact should have a license that at least allows use for comparison purposes</li>
</ul>

<p>Artifacts must meet these criteria <em>at the time of evaluation</em>.
Promises of future availability, such as artifacts “temporarily” gated behind credentials given to evaluators, are not enough.</p>

<h3 id="artifact-functional">Artifact Functional</h3>

<ul>
  <li>The artifact has a “read me” file with high-level documentation:
    <ul>
      <li>A description, such as which folders correspond to code, benchmarks, data, …</li>
      <li>A list of supported environments, including OS, specific hardware if necessary, …</li>
      <li>Compilation and running instructions, including dependencies and pre-installation steps,
with a reasonable degree of automation such as scripts to download and build exotic dependencies</li>
      <li>Configuration instructions, such as selecting IP addresses or disks</li>
      <li>Usage instructions, such as analyzing a new data set</li>
      <li>Instructions for a “minimal working example”</li>
    </ul>
  </li>
  <li>The artifact has documentation explaining the high-level organization of modules, and code comments explaining non-obvious code,
such that other researchers can fully understand it</li>
  <li>The artifact contains all components the paper describes using the same terminology as the paper, and no obsolete code/data</li>
  <li>If the artifact includes a container/VM, it must also contain a script to create it from scratch</li>
</ul>

<p>Artifacts must be usable on other machines than the authors’, though they may require hardware such as specific network cards.
Information such as IP addresses must not be hardcoded.</p>

<h3 id="results-reproduced">Results Reproduced</h3>

<ul>
  <li>The artifact has a “read me” file that documents:
    <ul>
      <li>The exact environment the authors used, including OS version and any special hardware</li>
      <li>The exact commands to run to reproduce each claim from the paper</li>
      <li>The approximate resources used per claim, such as “5 minutes, 1 GB of disk space”</li>
      <li>The scripts to reproduce claims are documented, allowing researchers to ensure they correspond to the claims;
merely producing the right output is not enough</li>
    </ul>
  </li>
  <li>The artifact’s external dependencies are fetched from well-known sources such as official websites or GitHub repositories
    <ul>
      <li>Changes to such dependencies should be clearly separated, such as using a patch file or a repository fork with a clear commit history</li>
    </ul>
  </li>
</ul>

<p>The amount of manual work, such as writing configuration files, should be reasonably minimized.
In particular, there should be no redundant manual steps such as writing the same configuration values in multiple places, as this inevitably leads to human error.</p>
